Q1: Can small language models (like Phi-2) perform comparably to large language models (LLMs) on RAG tasks when enhanced with high-quality chunking and retrieval?

Q2: How do different chunking techniques affect the performance of SLMs in a RAG setup?

Q3: How does performance vary across different model sizes given identical retriever inputs?